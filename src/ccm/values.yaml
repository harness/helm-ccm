global:
  istio:
    enabled: false
    gateway:
      create: false
    virtualService:
      gateways:
      hosts:

anomaly-detection:
  replicaCount: 2
  maxSurge: 100%
  maxUnavailable: 0
  java:
    memory: "4096m"
    memoryLimit: "4096m"
  image:
    registry: docker.io
    repository: harness/ce-anomaly-detection-signed
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "12"
    digest: ""
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    create: false
    annotations: {}
    name: "harness-default"
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  service:
    type: ClusterIP
    port: 8081
  ingress:
    className: nginx
  autoscaling:
    enabled: false
    minReplicas: 2
    maxReplicas: 4
    targetCPUUtilizationPercentage: 70
  nodeSelector: {}
  tolerations: []
  affinity: {}
  resources:
    limits:
      cpu: "1024m"
      memory: "2048Mi"
    requests:
      cpu: "1024m"
      memory: "2048Mi"

batch-processing:
  replicaCount: 1
  isolatedReplica: 0
  stackDriverLoggingEnabled: false
  java:
    memory: "7168"
  image:
    registry: docker.io
    repository: harness/batch-processing-signed
    pullPolicy: Always
    # Overrides the image tag whose default is the chart appVersion.
    tag: "78300"
    digest: ""
  waitForInitContainer:
    image:
      registry: docker.io
      repository: harness/helm-init-container
      pullPolicy: IfNotPresent
      tag: "latest"
      digest: ""
  mongoSecrets:
    userName:
      name: harness-secrets
      key: mongodbUsername
    password:
      name: mongodb-replicaset-chart
      key: mongodb-root-password
  timescaleSecret:
    password:
      name: harness-secrets
      key: timescaledbPostgresPassword
  smtp:
    host: ""
    password: ""
    user: ""
  ceBatchGCPCredentials: ""
  ceGCPHomeProjectCreds: ""
  storageObjectAdmin: ""
  awsSecret:
    S3_SYNC_CONFIG_ACCESSKEY: ""
    S3_SYNC_CONFIG_SECRETKEY: ""
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    create: false
    annotations: {}
    name: "harness-default"
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  service:
    type: ClusterIP
    port: 6340
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 2
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}
  cloudProviderConfig:
    GCP_PROJECT_ID: "placeHolder"
    DATA_PIPELINE_CONFIG_GCS_BASE_PATH: "gs://awscustomerbillingdata-onprem"
    CLUSTER_DATA_GCS_BUCKET: "clusterdata-onprem"
    CLUSTER_DATA_GCS_BACKUP_BUCKET: "clusterdata-onprem-backup"
    S3_SYNC_CONFIG_BUCKET_NAME: "ccm-service-data-bucket"
    S3_SYNC_CONFIG_REGION: "us-east-1"
  clickhouse:
    username: "default"
    password:
      name: clickhouse
      key: admin-password
  resources:
    limits:
      cpu: 1
      memory: "8192Mi"
    requests:
      cpu: 1
      memory: "8192Mi"

cloud-info:
  replicaCount: 2
  maxSurge: 100%
  maxUnavailable: 0
  java:
    memory: "4096m"
    memoryLimit: "4096m"
  image:
    registry: docker.io
    repository: harness/ce-cloud-info-signed
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "0.19.0"
    digest: ""
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    create: false
    annotations: {}
    name: "harness-default"
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  service:
    type: ClusterIP
    port: 8082
    targetPort: 8000
  ingress:
    className: nginx
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 2
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}
  CLOUD_INFO_GCP_CREDS: ""
  CLOUD_INFO_CONFIG: ""
  resources:
    limits:
      cpu: 2
      memory: "5120Mi"
    requests:
      cpu: 2
      memory: "5120Mi"

event-service:
  replicaCount: 2
  stackDriverLoggingEnabled: false
  maxSurge: 100%
  maxUnavailable: 0
  java:
    memory: 1024
  image:
    registry: docker.io
    repository: harness/event-service-signed
    pullPolicy: Always
    # Overrides the image tag whose default is the chart appVersion.
    tag: "77317"
    digest: ""
  waitForInitContainer:
    image:
      registry: docker.io
      repository: harness/helm-init-container
      pullPolicy: IfNotPresent
      tag: "latest"
      digest: ""
  eventSvcServiceAccounts:
    ceGCPHomeProjectCreds:
      name: placeHolderName
      key: placeHolderKey
  mongoSecrets:
    userName:
      name: harness-secrets
      key: mongodbUsername
    password:
      name: mongodb-replicaset-chart
      key: mongodb-root-password
  timescaleSecret:
    password:
      name: harness-secrets
      key: timescaledbPostgresPassword
  ce-gcp-home-project-creds: ""
  nameOverride: ""
  fullnameOverride: ""
  ngServiceAccount: "test"
  redislabsCATruststore: "test"
  defaultInternalImageConnector: "test"
  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: "harness-default"
  podAnnotations: {}
  podSecurityContext: {}
    # fsGroup: 2000
  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000
  service:
    type: ClusterIP
    port: 7280
    port2: 9889
  ingress:
    className: nginx
  resources:
    limits:
      cpu: "512m"
      memory: "1440Mi"
    requests:
      cpu: "512m"
      memory: "1440Mi"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 2
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}

lwd:
  replicaCount: 2
  maxSurge: 100%
  maxUnavailable: 0
  java:
    memory: "4096m"
    memoryLimit: "4096m"
  image:
    registry: docker.io
    repository: harness/lightwing-signed
    pullPolicy: IfNotPresent
    tag: "main-1.1.791"
    digest: ""
  waitForInitContainer:
    image:
      registry: docker.io
      repository: harness/helm-init-container
      pullPolicy: IfNotPresent
      tag: "latest"
      digest: ""
  postgresPassword:
    name: postgres
    key: postgres-password
  lwdSecrets:
    secretName: lwd-secrets
    faktoryPassword: FAKTORY_PASSWORD
    lightwingAwsmasterAccessKey: LIGHTWING_AWSMASTER_ACCESS_KEY
    lightwingAwsmasterSecretKey: LIGHTWING_AWSMASTER_SECRET_KEY
    lightwingAwsGovmasterAccessKey: LIGHTWING_AWS-GOV-MASTER_ACCESS_KEY
    lightwingAwsGovmasterSecretKey: LIGHTWING_AWS-GOV-MASTER_SECRET_KEY
    lightwingCloudConnectorAzureClientSecret: LIGHTWING_CLOUD-CONNECTOR_AZURE_CLIENT_SECRET
    lightwingMetricsSegment: LIGHTWING_METRICS_SEGMENT
  ce-batch-gcp-credentials: ""
  aws:
    region: "us-east-1"
  azure:
    clientId: ""
  redisUrl: "redis://localhost:6379"
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    create: false
    annotations: {}
    name: "harness-default"
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  service:
    type: ClusterIP
    port: 9090
  ingress:
    className: nginx
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 2
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}
  LIGHTWING_GCPCOST_PROJECT: "placeHolder"
  resources:
    limits:
      cpu: 2
      memory: "4Gi"
    requests:
      cpu: 2
      memory: "4Gi"

lwd-autocud:
  replicaCount: 2
  maxSurge: 100%
  maxUnavailable: 0
  java:
    memory: "4096m"
    memoryLimit: "4096m"
  image:
    registry: docker.io
    repository: harness/lightwing-signed
    pullPolicy: IfNotPresent
    tag: "main-1.1.564"
    digest: ""
  waitForInitContainer:
    image:
      registry: docker.io
      repository: harness/helm-init-container
      pullPolicy: IfNotPresent
      tag: "latest"
      digest: ""
  postgresPassword:
    name: postgres
    key: postgres-password
  lwdAutocudSecrets:
    secretName: lwd-secrets
    faktoryPassword: FAKTORY_PASSWORD
    lightwingAwsmasterAccessKey: LIGHTWING_AWSMASTER_ACCESS_KEY
    lightwingAwsmasterSecretKey: LIGHTWING_AWSMASTER_SECRET_KEY
    lightwingAwsGovmasterAccessKey: LIGHTWING_AWS-GOV-MASTER_ACCESS_KEY
    lightwingAwsGovmasterSecretKey: LIGHTWING_AWS-GOV-MASTER_SECRET_KEY
    lightwingCloudConnectorAzureClientSecret: LIGHTWING_CLOUD-CONNECTOR_AZURE_CLIENT_SECRET
    lightwingMetricsSegment: LIGHTWING_METRICS_SEGMENT
  ce-batch-gcp-credentials: ""
  aws:
    region: "us-east-1"
  azure:
    clientId: ""
  redisUrl: "redis://localhost:6379"
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    create: false
    annotations: {}
    name: "harness-default"
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  service:
    type: ClusterIP
    port: 9090
  ingress:
    className: nginx
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 2
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}
  LIGHTWING_GCPCOST_PROJECT: "placeHolder"
  resources:
    limits:
      cpu: 2
      memory: "4Gi"
    requests:
      cpu: 2
      memory: "4Gi"

lwd-faktory:
  replicaCount: 1
  image:
    registry: docker.io
    repository: contribsys/faktory
    pullPolicy: Always
    # Overrides the image tag whose default is the chart appVersion.
    tag: "1.6.1"
    digest: ""
  faktory:
    password:
      name: lwd-secrets
      key: FAKTORY_PASSWORD
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    create: false
    annotations: {}
    name: "harness-default"
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  service:
    type: ClusterIP
    dashboard:
      port: 7420
      name: "dashboard"
    api:
      port: 7419
      name: "api"
  ingress:
    className: nginx
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 2
    targetCPUUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}
  persistentVolume:
    storage:
      models: "20Gi"
    accessModes: "ReadWriteOnce"
  resources:
    limits:
      cpu: 2
      memory: "4Gi"
    requests:
      cpu: 2
      memory: "4Gi"

lwd-worker:
  replicaCount: 3
  maxSurge: 100%
  maxUnavailable: 0
  java:
    memory: "4096m"
    memoryLimit: "4096m"
  image:
    registry: docker.io
    repository: harness/lightwing-signed
    pullPolicy: IfNotPresent
    tag: "main-1.1.791"
    digest: ""
  waitForInitContainer:
    image:
      registry: docker.io
      repository: harness/helm-init-container
      pullPolicy: IfNotPresent
      tag: "latest"
      digest: ""
  postgresPassword:
    name: postgres
    key: postgres-password
  lwdWorkerSecrets:
    secretName: lwd-secrets
    faktoryPassword: FAKTORY_PASSWORD
    lightwingAwsmasterAccessKey: LIGHTWING_AWSMASTER_ACCESS_KEY
    lightwingAwsmasterSecretKey: LIGHTWING_AWSMASTER_SECRET_KEY
    lightwingAwsGovmasterAccessKey: LIGHTWING_AWS-GOV-MASTER_ACCESS_KEY
    lightwingAwsGovmasterSecretKey: LIGHTWING_AWS-GOV-MASTER_SECRET_KEY
    lightwingCloudConnectorAzureClientSecret: LIGHTWING_CLOUD-CONNECTOR_AZURE_CLIENT_SECRET
    lightwingMetricsSegment: LIGHTWING_METRICS_SEGMENT
  ce-batch-gcp-credentials: ""
  aws:
    region: "us-east-1"
  azure:
    clientId: ""
  redisUrl: "redis://localhost:6379"
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    create: false
    annotations: {}
    name: "harness-default"
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  service:
    type: ClusterIP
    port: 9090
  ingress:
    className: nginx
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 2
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}
  LIGHTWING_GCPCOST_PROJECT: "placeHolder"
  resources:
    limits:
      cpu: 2
      memory: "4Gi"
    requests:
      cpu: 2
      memory: "4Gi"

nextgen-ce:
  replicaCount: 2
  stackDriverLoggingEnabled: false
  maxSurge: 100%
  maxUnavailable: 0
  java:
    memory: "4096m"
    memoryLimit: "4096m"
  image:
    registry: docker.io
    repository: harness/ce-nextgen-signed
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "78400"
    digest: ""
  waitForInitContainer:
    image:
      registry: docker.io
      repository: harness/helm-init-container
      pullPolicy: IfNotPresent
      tag: "latest"
      digest: ""
  mongoSecrets:
    userName:
      name: harness-secrets
      key: mongodbUsername
    password:
      name: mongodb-replicaset-chart
      key: mongodb-root-password
  timescaleSecret:
    password:
      name: harness-secrets
      key: timescaledbPostgresPassword
  awsSecret:
    AWS_ACCESS_KEY: ""
    AWS_SECRET_KEY: ""
    AWS_ACCOUNT_ID: ""
    AWS_DESTINATION_BUCKET: ""
    AWS_TEMPLATE_LINK: ""
    CE_AWS_TEMPLATE_URL: ""
  ceng-gcp-credentials: ""
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    create: false
    annotations: {}
    name: "harness-default"
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  service:
    type: ClusterIP
    port: 6340
  ingress:
    className: nginx
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 2
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}
  cloudProviderConfig:
    GCP_PROJECT_ID: "placeHolder"
    AZURE_APP_CLIENT_ID: "0211763d-24fb-4d63-865d-92f86f77e908"
    AWS_GOV_CLOUD_ACCOUNT_ID: "147449478367"
    AWS_GOV_CLOUD_REGION_NAME: "us-gov-west-1"
    AWS_GOV_CLOUD_TEMPLATE_LINK: "https://continuous-efficiency.s3.us-east-2.amazonaws.com/setup/v1/ng/HarnessAWSTemplate.yaml"
  clickhouse:
    username: "default"
    password:
      name: clickhouse
      key: admin-password
  resources:
    limits:
      cpu: 2
      memory: "5120Mi"
    requests:
      cpu: 2
      memory: "5120Mi"

telescopes:
  replicaCount: 1
  java:
    memory: "4096m"
    memoryLimit: "4096m"
  image:
    registry: docker.io
    repository: harness/telescopes-signed
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "10100"
    digest: ""
  waitForInitContainer:
    image:
      registry: docker.io
      repository: harness/helm-init-container
      pullPolicy: IfNotPresent
      tag: "latest"
      digest: ""
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    create: false
    annotations: {}
    name: "harness-default"
  podAnnotations: {}
  podSecurityContext: {}
  securityContext: {}
  service:
    type: ClusterIP
    port: 9090
  ingress:
    className: nginx
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 2
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}
  resources:
    limits:
      cpu: "512m"
      memory: "1024Mi"
    requests:
      cpu: "512m"
      memory: "1024Mi"

clickhouse:
  enabled: false
  fullnameOverride: "clickhouse"
  image:
    tag: 22.11.2-debian-11-r0
  resources:
    limits:
      cpu: "16000m"
      memory: "6Gi"
    requests:
      cpu: "16000m"
      memory: "6Gi"
  persistence:
    size: 1Ti
  defaultConfigurationOverrides: |
    <clickhouse>
      <!-- Macros -->
      <macros>
        <shard from_env="CLICKHOUSE_SHARD_ID"></shard>
        <replica from_env="CLICKHOUSE_REPLICA_ID"></replica>
        <layer>{{ include "common.names.fullname" . }}</layer>
      </macros>
      <!-- Log Level -->
      <logger>
        <level>{{ .Values.logLevel }}</level>
      </logger>
      {{- if or (ne (int .Values.shards) 1) (ne (int .Values.replicaCount) 1)}}
      <!-- Cluster configuration - Any update of the shards and replicas requires helm upgrade -->
      <remote_servers>
        <default>
          {{- $shards := $.Values.shards | int }}
          {{- range $shard, $e := until $shards }}
          <shard>
              <internal_replication>false</internal_replication>
              {{- $replicas := $.Values.replicaCount | int }}
              {{- range $i, $_e := until $replicas }}
              <replica>
                  <host>{{ printf "%s-shard%d-%d.%s.%s.svc.%s" (include "common.names.fullname" $ ) $shard $i (include "clickhouse.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
                  <port>{{ $.Values.service.ports.tcp }}</port>
              </replica>
              {{- end }}
          </shard>
          {{- end }}
        </default>
      </remote_servers>
      {{- end }}
      {{- if .Values.keeper.enabled }}
      <!-- keeper configuration -->
      <keeper_server>
        {{/*ClickHouse keeper configuration using the helm chart */}}
        <tcp_port>{{ $.Values.containerPorts.keeper }}</tcp_port>
        {{- if .Values.tls.enabled }}
        <tcp_port_secure>{{ $.Values.containerPorts.keeperSecure }}</tcp_port_secure>
        {{- end }}
        <server_id from_env="KEEPER_SERVER_ID"></server_id>
        <log_storage_path>/bitnami/clickhouse/keeper/coordination/log</log_storage_path>
        <snapshot_storage_path>/bitnami/clickhouse/keeper/coordination/snapshots</snapshot_storage_path>

        <coordination_settings>
            <operation_timeout_ms>10000</operation_timeout_ms>
            <session_timeout_ms>30000</session_timeout_ms>
            <raft_logs_level>trace</raft_logs_level>
        </coordination_settings>

        <raft_configuration>
        {{- $nodes := .Values.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <server>
          <id>{{ $node | int }}</id>
          <hostname from_env="{{ printf "KEEPER_NODE_%d" $node }}"></hostname>
          <port>{{ $.Values.service.ports.keeperInter }}</port>
        </server>
        {{- end }}
        </raft_configuration>
      </keeper_server>
      {{- end }}
      {{- if or .Values.keeper.enabled .Values.zookeeper.enabled .Values.externalZookeeper.servers }}
      <!-- Zookeeper configuration -->
      <zookeeper>
        {{- if or .Values.keeper.enabled }}
        {{- $nodes := .Values.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <node>
          <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
          <port>{{ $.Values.service.ports.keeper }}</port>
        </node>
        {{- end }}
        {{- else if .Values.zookeeper.enabled }}
        {{/* Zookeeper configuration using the helm chart */}}
        {{- $nodes := .Values.zookeeper.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <node>
          <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
          <port>{{ $.Values.zookeeper.service.ports.client }}</port>
        </node>
        {{- end }}
        {{- else if .Values.externalZookeeper.servers }}
        {{/* Zookeeper configuration using an external instance */}}
        {{- range $node :=.Values.externalZookeeper.servers }}
        <node>
          <host>{{ $node }}</host>
          <port>{{ $.Values.externalZookeeper.port }}</port>
        </node>
        {{- end }}
        {{- end }}
      </zookeeper>
      {{- end }}
      {{- if .Values.tls.enabled }}
      <!-- TLS configuration -->
      <tcp_port_secure from_env="CLICKHOUSE_TCP_SECURE_PORT"></tcp_port_secure>
      <openSSL>
          <server>
              {{- $certFileName := default "tls.crt" .Values.tls.certFilename }}
              {{- $keyFileName := default "tls.key" .Values.tls.certKeyFilename }}
              <certificateFile>/bitnami/clickhouse/certs/{{$certFileName}}</certificateFile>
              <privateKeyFile>/bitnami/clickhouse/certs/{{$keyFileName}}</privateKeyFile>
              <verificationMode>none</verificationMode>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              {{- if or .Values.tls.autoGenerated .Values.tls.certCAFilename }}
              {{- $caFileName := default "ca.crt" .Values.tls.certFilename }}
              <caConfig>/bitnami/clickhouse/certs/{{$caFileName}}</caConfig>
              {{- else }}
              <loadDefaultCAFile>true</loadDefaultCAFile>
              {{- end }}
          </server>
          <client>
              <loadDefaultCAFile>true</loadDefaultCAFile>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              <verificationMode>none</verificationMode>
              <invalidCertificateHandler>
                  <name>AcceptCertificateHandler</name>
              </invalidCertificateHandler>
          </client>
      </openSSL>
      {{- end }}
      {{- if .Values.metrics.enabled }}
      <!-- Prometheus metrics -->
      <prometheus>
          <endpoint>/metrics</endpoint>
          <port from_env="CLICKHOUSE_METRICS_PORT"></port>
          <metrics>true</metrics>
          <events>true</events>
          <asynchronous_metrics>true</asynchronous_metrics>
      </prometheus>
      {{- end }}
    </clickhouse>
  replicaCount: 1
  shards: 1
  
  zookeeper:
    enabled: true
    fullnameOverride: "clickhouse-zookeeper"
    replicaCount: 1
